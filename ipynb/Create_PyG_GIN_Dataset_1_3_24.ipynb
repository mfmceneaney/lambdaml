{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "greater-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------#\n",
    "# Lambda event classification dataset creation script\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# I/O\n",
    "import uproot as ur\n",
    "import hipopy.hipopy as hp # <--- Package for reading in the hipo files\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Physics\n",
    "from particle import PDGID\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "import sys #NOTE: ADDED\n",
    "import tqdm\n",
    "\n",
    "# ML Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch_geometric as tg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "running-exclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_decay_test =  False\n",
      "rec_indices    =  [-1, -1]\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------#\n",
    "# HIPO bank reading and linking functions\n",
    "\n",
    "def get_bank_keys(bank_name,all_keys,separator='_'):\n",
    "    \"\"\"\n",
    "    :description: Get list of the keys for given bank name from a list of all batch keys.\n",
    "    \n",
    "    :param: bank_name\n",
    "    :param: all_keys\n",
    "    :param: separator='_'\n",
    "    \n",
    "    :return: bank_keys\n",
    "    \"\"\"\n",
    "    bank_keys = []\n",
    "    for key in all_keys:\n",
    "        if key.startswith(bank_name+separator):\n",
    "            bank_keys.append(key)\n",
    "    return bank_keys\n",
    "        \n",
    "def get_event_table(bank_keys,event_num,batch,dtype=float):\n",
    "    \"\"\"\n",
    "    :description: Get a bank event table as a numpy array of shape (number of columns, number of rows).\n",
    "    \n",
    "    :param: bank_keys\n",
    "    :param: event_num\n",
    "    :param: batch\n",
    "    :param: dtype=float\n",
    "    \n",
    "    :return: bank_table\n",
    "    \"\"\"\n",
    "    bank_table = []\n",
    "    bank_table = np.moveaxis(np.array([batch[key][event_num] for key in bank_keys], dtype=dtype),[0,1],[1,0])\n",
    "    return bank_table\n",
    "\n",
    "def get_link_indices(event_table_rec_particle,event_table,pindex_idx=1):\n",
    "    \"\"\"\n",
    "    :description: Get index pairs linking entries in a bank back to entries in the 'REC::Particle' bank.\n",
    "    \n",
    "    :param: event_table_rec_particle\n",
    "    :param: event_table\n",
    "    :param: pindex_idx=1\n",
    "    \n",
    "    :return: link_indices\n",
    "    \"\"\"\n",
    "    \n",
    "    link_indices = []\n",
    "    nrec = np.shape(event_table_rec_particle)[0]\n",
    "    for rec_particle_idx in range(0,nrec):\n",
    "        for event_table_idx, el in enumerate(event_table[:,pindex_idx]):\n",
    "            if el==rec_particle_idx:\n",
    "                link_indices.append([rec_particle_idx,event_table_idx])\n",
    "    return np.array(link_indices,dtype=int) #NOTE: link_indices = [(event_table_idx,rec_particle_idx)]\n",
    "\n",
    "def get_parent_indices(mc_event_table,index_idx=0,parent_idx=4,daughter_idx=5):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    for mc_event_table_idx, index in enumerate(mc_event_table[:,index_idx]):\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "def get_match_indices(\n",
    "    rec_event_table,\n",
    "    mc_event_table,\n",
    "    rec_px_idx             = 1,\n",
    "    rec_py_idx             = 2,\n",
    "    rec_pz_idx             = 3,\n",
    "    rec_ch_idx             = 8,\n",
    "    mc_px_idx              = 6,\n",
    "    mc_py_idx              = 7,\n",
    "    mc_pz_idx              = 8,\n",
    "    mc_pid_idx             = 3,\n",
    "    mc_daughter_idx        = 5,\n",
    "    match_charge           = True,\n",
    "    require_no_mc_daughter = True,\n",
    "    enforce_uniqueness     = True,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    :description: Get index pairs matching \n",
    "    \n",
    "    :param: rec_event_table\n",
    "    :param: mc_event_table\n",
    "    :param: rec_px_idx             = 1,\n",
    "    :param: rec_py_idx             = 2,\n",
    "    :param: rec_pz_idx             = 3,\n",
    "    :param: rec_ch_idx             = 8,\n",
    "    :param: mc_px_idx              = 6,\n",
    "    :param: mc_py_idx              = 7,\n",
    "    :param: mc_pz_idx              = 8,\n",
    "    :param: mc_pid_idx             = 3,\n",
    "    :param: mc_daughter_idx        = 5,\n",
    "    :param: match_charge           = True,\n",
    "    :param: require_no_mc_daughter = True,\n",
    "    :param: enforce_uniqueness     = True,\n",
    "    \n",
    "    :return: match_indices\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set minimum\n",
    "    rec_final_state_min_idx = 1\n",
    "    mc_final_state_min_idx  = 3 #NOTE: MC::Lund bank is structured [e, p, q, e', all the other final state particles...]\n",
    "    \n",
    "    # Initialize index map\n",
    "    match_indices    = -np.ones((rec_event_table.shape[0],2),dtype=float)\n",
    "    match_indices[0] = [0,3] #NOTE: Always match first entry in REC::Particle to scattered electron in MC::Lund.\n",
    "\n",
    "    # Get REC::Particle info\n",
    "    rec_px    = rec_event_table[:,rec_px_idx]\n",
    "    rec_py    = rec_event_table[:,rec_py_idx]\n",
    "    rec_pz    = rec_event_table[:,rec_pz_idx]\n",
    "    rec_pT    = np.sqrt(np.square(rec_event_table[:,rec_px_idx])+np.square(rec_event_table[:,rec_py_idx]))\n",
    "    rec_p     = np.sqrt(np.square(rec_event_table[:,rec_px_idx])+np.square(rec_event_table[:,rec_py_idx])+np.square(rec_event_table[:,rec_pz_idx]))\n",
    "    rec_theta = np.array(rec_pz)\n",
    "    rec_theta = np.arctan(rec_pT,rec_theta)\n",
    "    rec_phi   = np.arctan2(rec_py,rec_px)\n",
    "    \n",
    "    # Get MC::Lund info\n",
    "    mc_px    = mc_event_table[:,mc_px_idx]\n",
    "    mc_py    = mc_event_table[:,mc_py_idx]\n",
    "    mc_pz    = mc_event_table[:,mc_pz_idx]\n",
    "    mc_pT    = np.sqrt(np.square(mc_event_table[:,mc_px_idx])+np.square(mc_event_table[:,mc_py_idx]))\n",
    "    mc_p     = np.sqrt(np.square(mc_event_table[:,mc_px_idx])+np.square(mc_event_table[:,mc_py_idx])+np.square(mc_event_table[:,mc_pz_idx]))\n",
    "    mc_theta = np.array(mc_pz)\n",
    "    mc_theta = np.arctan(mc_pT,mc_theta)\n",
    "    mc_phi   = np.arctan2(mc_py,mc_px)\n",
    "\n",
    "    # Loop rec particles\n",
    "    for rec_idx, rec_part in enumerate(rec_event_table):\n",
    "        \n",
    "        # Start with final state particles past scattered electron\n",
    "        if rec_idx<rec_final_state_min_idx: continue\n",
    "        \n",
    "        # Get REC::Particle charge\n",
    "        rec_ch = rec_event_table[rec_idx,rec_ch_idx]\n",
    "        \n",
    "        # Loop mc particles\n",
    "        mc_match_idx = -1\n",
    "        min_domega   = 9999\n",
    "        for mc_idx, mc_part in enumerate(mc_event_table):\n",
    "            \n",
    "            # Start with final state particles past scattered electron\n",
    "            if mc_idx<mc_final_state_min_idx:\n",
    "                continue\n",
    "            \n",
    "            # Enforce unique matching\n",
    "            if enforce_uniqueness and mc_idx in match_indices[:,1]:\n",
    "                continue\n",
    "            \n",
    "            # Match charge and require that the MC particle be final state (no daughters)\n",
    "            if match_charge and rec_ch!=PDGID(mc_event_table[mc_idx,mc_pid_idx]).charge:\n",
    "                continue\n",
    "            if require_no_mc_daughter and mc_event_table[mc_idx,mc_daughter_idx]!=0:\n",
    "                continue\n",
    "                \n",
    "            # Get angular and momentum differences\n",
    "            dp     = np.abs(rec_p[rec_idx]     - mc_p[mc_idx])\n",
    "            dtheta = np.abs(rec_theta[rec_idx] - mc_theta[mc_idx])\n",
    "            dphi   = np.abs(rec_phi[rec_idx]   - mc_phi[mc_idx]) if np.abs(rec_phi[rec_idx] - mc_phi[mc_idx])<np.pi else 2*np.pi-np.abs(rec_phi[rec_idx] - mc_phi[mc_idx])\n",
    "            domega = dp**2 + dtheta**2 + dphi**2\n",
    "            \n",
    "            # Reset angular, momentum minimum difference\n",
    "            if domega<min_domega:\n",
    "                min_domega   = domega\n",
    "                mc_match_idx = mc_idx\n",
    "                \n",
    "        # Append matched index pair\n",
    "        match_indices[rec_idx] = [rec_idx,mc_match_idx]\n",
    "        \n",
    "    return np.array(match_indices,dtype=int) #NOTE: IMPORTANT!\n",
    "\n",
    "def get_info(base_indices,link_indices,bank_entry_indices,bank_event_table):\n",
    "    \"\"\"\n",
    "    :description: Get selected entry info from other banks linked to REC::Particle.\n",
    "    \n",
    "    :param: base_indices\n",
    "    :param: link_indices #NOTE: if None assume bank is REC::Particle and use identity map\n",
    "    :param: bank_entry_indices\n",
    "    :param: bank_event_table\n",
    "    \n",
    "    :return: bank_info as awkward.Array\n",
    "    \"\"\"\n",
    "    if link_indices is None:\n",
    "        bank_info = []\n",
    "        for base_idx in base_indices:\n",
    "            base_info = bank_event_table[base_idx,bank_entry_indices]\n",
    "            bank_info.append([base_info])\n",
    "            \n",
    "        return ak.Array(bank_info)\n",
    "            \n",
    "    bank_info = []\n",
    "    for base_idx in base_indices:\n",
    "        base_info = []\n",
    "        for rec_particle_idx, link_idx in link_indices:\n",
    "            if rec_particle_idx==base_idx:\n",
    "                base_info.append(bank_event_table[link_idx,bank_entry_indices]) #NOTE: INDICES HAVE TO BE INTEGERS...COULD ADD CHECK...\n",
    "        if len(base_info)==0: #NOTE: Address case that no matches exist between banks\n",
    "            base_info.append(np.zeros((len(bank_entry_indices),)))\n",
    "        bank_info.append(base_info)\n",
    "    \n",
    "    return ak.Array(bank_info)\n",
    "\n",
    "def get_truth_info(base_indices,match_indices,truth_entry_indices,mc_event_table):\n",
    "    \"\"\"\n",
    "    :description: Get selected entry info from other banks linked to REC::Particle.\n",
    "    \n",
    "    :param: base_indices\n",
    "    :param: link_indices #NOTE: if None assume bank is REC::Particle and use identity map\n",
    "    :param: bank_entry_indices\n",
    "    :param: bank_event_table\n",
    "    \n",
    "    :return: bank_info as awkward.Array\n",
    "    \"\"\"\n",
    "    \n",
    "    bank_info = []\n",
    "    for base_idx in base_indices:\n",
    "        base_info = []\n",
    "        for rec_particle_idx, match_idx in match_indices:\n",
    "            if rec_particle_idx==base_idx:\n",
    "                base_info.append(mc_event_table[match_idx,truth_entry_indices]) #NOTE: INDICES HAVE TO BE INTEGERS...COULD ADD CHECK...\n",
    "        if len(base_info)==0: #NOTE: Address case that no matches exist between banks\n",
    "            base_info.append(np.zeros((len(truth_entry_indices),)))\n",
    "        bank_info.append(base_info)\n",
    "    \n",
    "    return ak.Array(bank_info)\n",
    "\n",
    "def check_has_decay(rec_particle_event_table,mc_lund_event_table,match_indices,decay,\n",
    "                    rec_particle_pid_idx=0,mc_lund_pid_idx=3,mc_lund_parent_idx=4,mc_lund_daughter_idx=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    :description: Check if specified decay is present in MC::Lund bank and the decay daughters are\n",
    "        matched to particles of the same pid in REC::Particle\n",
    "    \n",
    "    :param: rec_particle_event_table\n",
    "    :param: mc_lund_event_table\n",
    "    :param: match_indices\n",
    "    :param: decay\n",
    "    :param: rec_particle_pid_idx = 0\n",
    "    :param: mc_lund_pid_idx      = 3\n",
    "    :param: mc_lund_parent_idx   = 4\n",
    "    :param: mc_lund_daughter_idx = 5\n",
    "    \n",
    "    :return: boolean has_decay\n",
    "    \"\"\"\n",
    "    \n",
    "    has_decay = False\n",
    "    decay_indices = None\n",
    "    rec_indices = [-1 for i in range(len(decay[1]))] #NOTE: This assumes a 1-level decay...#TODO: Write more robust algorithm.\n",
    "    if np.max(match_indices[:,-1])==-1: return has_decay, rec_indices #NOTE: This is the case of no matches at all.\n",
    "    \n",
    "    # Check if parent pid is in MC::Lund\n",
    "    if not decay[0] in mc_lund_event_table[:,mc_lund_pid_idx]: return has_decay, rec_indices\n",
    "    \n",
    "    # Loop MC::Lund to find parent\n",
    "    for parent_idx, parent_pid in enumerate(mc_lund_event_table[:,mc_lund_pid_idx]):\n",
    "        if parent_pid==decay[0]:\n",
    "#             print(\"DEBUGGING: found parent_pid == \",decay[0])\n",
    "            \n",
    "            # Check daughter pids in MC::Lund\n",
    "            daughter_idx     = int(mc_lund_event_table[parent_idx,mc_lund_daughter_idx])-1 #NOTE: -1 is important because Lund index begins at 1.\n",
    "            try:\n",
    "                daughter_pids    = mc_lund_event_table[daughter_idx:daughter_idx+len(decay[1]),mc_lund_pid_idx]\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"DEBUGGING: np.shape(mc_lund_event_table) = \",np.shape(mc_lund_event_table))\n",
    "                print(\"DEBUGGING: daughter_idx        = \",daughter_idx)\n",
    "                print(\"DEBUGGING: decay = \",decay)\n",
    "                raise Exception\n",
    "            daughter_parents = mc_lund_event_table[daughter_idx:daughter_idx+len(decay[1]),mc_lund_parent_idx]\n",
    "            daughter_parents -= 1 #NOTE: Important because Lund index begins at 1.\n",
    "            if np.all(daughter_parents==parent_idx) and np.all(daughter_pids==decay[1]): #NOTE: this relies on input arrays being np...\n",
    "                decay_indices = [parent_idx,[daughter_idx+i for i in range(len(decay[1]))]]\n",
    "                rec_indices = [-1 for i in range(len(decay[1]))]#NOTE: Reset in case you have two not fully matched decays...unlikely but possible.\n",
    "                \n",
    "                # Now check that there is a match of the same pid in REC::Particle\n",
    "                num_matched_daughters = 0\n",
    "                num_daughters         = len(decay[1])\n",
    "                for decay_idx, mc_idx in enumerate(decay_indices[1]):\n",
    "                    for el in match_indices:\n",
    "                        if el[1]==mc_idx:\n",
    "                            rec_idx = el[0]\n",
    "                            if rec_particle_event_table[rec_idx,rec_particle_pid_idx] == decay[1][decay_idx]:\n",
    "                                num_matched_daughters += 1\n",
    "                                rec_indices[decay_idx] = rec_idx #NOTE: That this is the actual index beginning at 0.\n",
    "#                 print(\"DEBUGGING: num_matched_daughters = \",num_matched_daughters)\n",
    "#                 print(\"DEBUGGING: decay_indices = \",decay_indices)\n",
    "                if num_matched_daughters == num_daughters:\n",
    "                    has_decay = True\n",
    "    \n",
    "    return has_decay, rec_indices\n",
    "\n",
    "# Select requested rows from REC::Particle data\n",
    "def get_sub_array(arr,indices):\n",
    "    \n",
    "    \"\"\"\n",
    "    :description: Get sub array at indices along axis 1.\n",
    "    \n",
    "    :param: arr\n",
    "    :param: indices\n",
    "    \n",
    "    :return: np.array new_arr\n",
    "    \"\"\"\n",
    "    \n",
    "    new_array = []\n",
    "    for i in indices:\n",
    "        new_array.append(arr[:,i])\n",
    "    new_array = np.moveaxis(np.array(new_array),[0,1],[1,0])\n",
    "\n",
    "    return new_array\n",
    "\n",
    "def replace_pids(arr,pid_map,pid_i=0):\n",
    "    \"\"\"\n",
    "    :description: Replace pids in given array roughly following scheme described in arxiv:1810.05165.\n",
    "    \n",
    "    :param: arr masked ndarray with dtype=float\n",
    "    :param: pid_i last depth index for pids in arr\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'int' in str(arr.dtype):\n",
    "        print(\" *** ERROR *** array passed to replace_pids should not have dtype==int\")\n",
    "        return\n",
    "    \n",
    "    for key in pid_map:\n",
    "        arr[:,pid_i] = np.where(arr[:,pid_i]==key,\n",
    "                                  pid_map[key],\n",
    "                                  arr[:,pid_i])\n",
    "\n",
    "def preprocess(x):\n",
    "    \"\"\"\n",
    "    :description: Run preprocessing on input array.\n",
    "    \n",
    "    :param: x\n",
    "    \n",
    "    :return: new_x\n",
    "    \"\"\"\n",
    "    #NOTE: Assume indices go 0-6 : pid, px, py, pz, beta, chi2pid, status\n",
    "    \n",
    "    pid_map = {\n",
    "        22:0.0,\n",
    "        11:-1.0,\n",
    "        -11:1.0,\n",
    "        2212:0.8,\n",
    "        -2212:-0.8,\n",
    "        2112:0.5,\n",
    "        111:0.1,\n",
    "        211:0.6,\n",
    "        -211:-0.6,\n",
    "        311:0.3,\n",
    "        321:0.4,\n",
    "        -321:-0.4,\n",
    "        45:0.0\n",
    "    }\n",
    "    \n",
    "    pid_idx, px_idx, py_idx, pz_idx, beta_idx, chi2pid_idx, status_idx = [i for i in range(np.shape(x)[-1])]\n",
    "    \n",
    "    # Reassign PIDs\n",
    "    replace_pids(x,pid_map,pid_i=0)\n",
    "    \n",
    "    # Compute new arrays\n",
    "    pT    = np.sqrt(np.add(np.square(x[:,px_idx]),np.square(x[:,py_idx])))\n",
    "    phi   = np.arctan2(x[:,py_idx],x[:,px_idx])\n",
    "    theta = np.divide(pT,x[:,pz_idx])\n",
    "    beta  = np.log(x[:,beta_idx])\n",
    "    \n",
    "    #TODO: Compute event level differences and normalize: pT, phi, theta, beta (apply lognorm first though...)\n",
    "    pT    -= pT.mean()\n",
    "    pT    /= np.abs(pT).max() if np.abs(pT).max() > 0.0 else 1.0\n",
    "    phi   -= phi.mean()\n",
    "    phi   /= np.abs(phi).max() if np.abs(phi).max() > 0.0 else 1.0\n",
    "    theta -= theta.mean()\n",
    "    theta /= np.abs(theta).max() if np.abs(theta).max() > 0.0 else 1.0\n",
    "    beta  -= beta.mean()\n",
    "    beta  /= np.abs(beta).max() if np.abs(beta).max() > 0.0 else 1.0\n",
    "    \n",
    "    # Reset px, py, pz, beta -> pT, phi, theta, beta\n",
    "    x[:,px_idx]   = pT\n",
    "    x[:,py_idx]   = phi\n",
    "    x[:,pz_idx]   = theta\n",
    "    x[:,beta_idx] = beta\n",
    "    \n",
    "#     # Preprocess pT, phi, theta\n",
    "#     x[:,px_idx] /= 10.0\n",
    "#     x[:,py_idx] /= np.pi\n",
    "#     x[:,pz_idx] /= np.pi\n",
    "    \n",
    "#     # Preprocess beta variable\n",
    "#     x[:,beta_idx] -= 1\n",
    "    \n",
    "    # Preprocess chi2pid\n",
    "    chi2_max, chi2_default, chi2_replacement   = 10, 9999, 10 #NOTE: DEBUGGING: USED TO BE >=chi2_default below!\n",
    "    x[:,chi2pid_idx]  = np.where(x[:,chi2pid_idx]>=chi2_default, chi2_replacement, x[:,chi2pid_idx])\n",
    "    x[:,chi2pid_idx] /= chi2_max #NOTE: IMPORTANT!  NEEDS TO BE AFTER REPLACEMENT OF MAX ABOVE!\n",
    "    \n",
    "    # Preprocess status variable\n",
    "    x[:,status_idx] /= 5000\n",
    "    \n",
    "    # Reassign all data values that are NaN or Inf to zero\n",
    "    x = np.where(np.isnan(x), 0.0, x)\n",
    "    x = np.where(np.isinf(x), 0.0, x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def preprocess_rec_traj(x):\n",
    "    \"\"\"\n",
    "    :description: Run preprocessing on input array assuming it is full matrix of REC::Traj bank.\n",
    "    \n",
    "    :param: x\n",
    "    \n",
    "    :return: new_x\n",
    "    \"\"\"\n",
    "    #NOTE: Assume indices go 0-10 : pindex, index, detector, layer, x, y, z, cx, cy, cz, path\n",
    "    \n",
    "    pindex_idx, index_idx, detector_idx, layer_idx, x_idx, y_idx, z_idx, cx_idx, cy_idx, cz_idx, path_idx = [i for i in range(np.shape(x)[-1])]\n",
    "    \n",
    "    # Preprocess row info\n",
    "#     x[:,x_idx]    -= 0.0\n",
    "    x[:,x_idx]    /= 1500.0\n",
    "#     x[:,y_idx]    -= 0.0\n",
    "    x[:,y_idx]    /= 1500.0\n",
    "#     x[:,z_idx]    -= 0.0\n",
    "    x[:,z_idx]    /= 1500.0\n",
    "#     x[:,cx_idx]   -= 0.0\n",
    "#     x[:,cx_idx]   /= 1.0\n",
    "#     x[:,cy_idx]   -= 0.0\n",
    "#     x[:,cy_idx]   /= 1.0\n",
    "#     x[:,cz_idx]   -= 0.0\n",
    "#     x[:,cz_idx]   /= 1.0\n",
    "#     x[:,path_idx] -= 0.0\n",
    "    x[:,path_idx] /= 1500.0\n",
    "    \n",
    "    # Select rows to take #NOTE: Do this after preprocessing so indices make sense\n",
    "    indices = [x_idx, y_idx, z_idx, cx_idx, cy_idx, cz_idx, path_idx]\n",
    "    x = np.take(x,indices,axis=1)\n",
    "    \n",
    "    # Reassign all data values that are NaN or Inf to zero\n",
    "    x = np.where(np.isnan(x), 0.0, x)\n",
    "    x = np.where(np.isinf(x), 0.0, x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Test check_has_decay() function\n",
    "rec_particle_event_table = np.array([[11],[2212],[-211],[-211],[321],[22],[22],[22]])\n",
    "mc_lund_event_table = np.array([[11,0,0],[2212,0,0],[22,0,0],[11,0,0],[3122,0,6],[2212,5,0],[-211,5,0],[-211,0,0],[-321,0,0],[22,0,0],[22,0,0],[22,0,0]])\n",
    "match_indices = np.array([[0,3],[1,5],[2,6],[3,7],[4,8],[5,9],[6,10],[7,11]])#NOTE: ALL MATCH\n",
    "match_indices = np.array([[0,3],[1,-1],[2,6],[3,7],[4,8],[5,9],[6,10],[7,11]])#NOTE: NO PROTON MATCH\n",
    "match_indices = np.array([[0,3],[1,5],[2,-1],[3,-1],[4,8],[5,9],[6,10],[7,11]])#NOTE: NO PION MATCH\n",
    "match_indices = np.array([[0,3],[1,5],[2,8],[3,-1],[4,-1],[5,9],[6,10],[7,11]])#NOTE: MATCH PION1 TO KAON, DON'T MATCH PION2 OR KAON\n",
    "match_indices = np.array([[0,3],[1,10],[2,8],[3,-1],[4,-1],[5,9],[6,-1],[7,11]])#NOTE: MATCH PION1 TO KAON, MATCH PROTON TO PHOTON, DON'T MATCH PION2 OR KAON\n",
    "decay = [3122,[2212,-211]] #NOTE: ORDERING MATTERS HERE, MUST MATCH ORDER (charge and then mass I think) in MC::Lund bank.\n",
    "has_decay_test, rec_indices = check_has_decay(rec_particle_event_table,mc_lund_event_table,match_indices,decay,\n",
    "                                 rec_particle_pid_idx=0,mc_lund_pid_idx=0,mc_lund_parent_idx=1,mc_lund_daughter_idx=2)\n",
    "print(\"has_decay_test = \",has_decay_test)\n",
    "print(\"rec_indices    = \",rec_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "defined-royalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:32,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUGGING: torch.where(torch.abs(x)>1.0) =  (tensor([48]), tensor([6]))\n",
      "DEBUGGING: torch.abs(x).max()            =  tensor(1.0063)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [01:05,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUGGING: torch.where(torch.abs(x)>1.0) =  (tensor([26]), tensor([6]))\n",
      "DEBUGGING: torch.abs(x).max()            =  tensor(1.0131)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [01:17,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUGGING: torch.where(torch.abs(x)>1.0) =  (tensor([26]), tensor([6]))\n",
      "DEBUGGING: torch.abs(x).max()            =  tensor(1.0064)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [01:29,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUGGING: torch.where(torch.abs(x)>1.0) =  (tensor([48]), tensor([6]))\n",
      "DEBUGGING: torch.abs(x).max()            =  tensor(1.0042)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [01:45,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUGGING: torch.where(torch.abs(x)>1.0) =  (tensor([66]), tensor([6]))\n",
      "DEBUGGING: torch.abs(x).max()            =  tensor(1.0193)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [03:10,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUGGING: torch.where(torch.abs(x)>1.0) =  (tensor([37]), tensor([6]))\n",
      "DEBUGGING: torch.abs(x).max()            =  tensor(1.0088)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [03:15,  4.00s/it]\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------#\n",
    "# File iteration\n",
    "\n",
    "# Set input files, banks, and step size\n",
    "file_list = [\n",
    "     '/volatile/clas12/users/mfmce/mc_jobs_rga_vtx_3_23_23/analysis2/kinematics_out_skim_50nA_OB_job_3313_0.hipo'\n",
    "]\n",
    "banks = [\n",
    "    'REC::Particle',\n",
    "    'REC::Traj',\n",
    "    'REC::Kinematics',\n",
    "    'MC::Lund',\n",
    "]\n",
    "step  = 1000\n",
    "\n",
    "# Set padding dimension (gets multiplied by len(rec_traj_keys))\n",
    "max_linked_entries = 30\n",
    "\n",
    "# Set output dataset and path names\n",
    "#TODO\n",
    "\n",
    "# Set entries to use as data/truth\n",
    "rec_particle_entry_indices   = [0,1,2,3,9,10,11] # pid, px, py, pz, (vx, vy, vz, vt (Only in MC?), charge), beta, chi2pid, status #TODO: SET THESE OUTSIDE LOOPS\n",
    "rec_kinematics_entry_indices = [i for i in range(11)] # idxe, idxp, idxpi, Q2, nu, W, x, y, z, xF, mass\n",
    "truth_entry_indices          = [3,4,5] # pid, parent, daughter #TODO: SET THESE OUTSIDE LOOPS\n",
    "\n",
    "# Iterate hipo file\n",
    "datalist = []\n",
    "for batch_num, batch in tqdm.tqdm(enumerate(hp.iterate(file_list,banks=banks,step=step))):\n",
    "    \n",
    "    # Set bank names and entry names to look at\n",
    "    all_keys            = list(batch.keys())\n",
    "    rec_particle_name   = 'REC::Particle'\n",
    "    rec_particle_keys   = get_bank_keys(rec_particle_name,all_keys)\n",
    "    rec_traj_name       = 'REC::Traj'\n",
    "    rec_traj_keys       = get_bank_keys(rec_traj_name,all_keys)\n",
    "    rec_kinematics_name = 'REC::Kinematics'\n",
    "    rec_kinematics_keys = get_bank_keys(rec_kinematics_name,all_keys)\n",
    "    mc_lund_name        = 'MC::Lund'\n",
    "    mc_lund_keys        = get_bank_keys(mc_lund_name,all_keys)\n",
    "    \n",
    "    # Loop events in batch\n",
    "    for event_num, _ in enumerate(range(0,len(batch[list(batch.keys())[0]]))):\n",
    "        \n",
    "        # Check scattered electron and proton pion in REC::Particle #NOTE: Should prefilter events anyway but check just in case.\n",
    "        filter_pids  = [2212,-211]\n",
    "        rec_pid_idx  = 0\n",
    "        no_filter_pids = True\n",
    "        count_filter_pids = 0\n",
    "        for pid in filter_pids:\n",
    "            if pid in batch['REC::Particle_pid'][event_num]: count_filter_pids += 1\n",
    "        if count_filter_pids == len(filter_pids): no_filter_pids = False\n",
    "        if batch['REC::Particle_pid'][event_num][0]!=11 or no_filter_pids: continue #NOTE: Check that particles of interest actually present in event\n",
    "        \n",
    "        # Get REC::Particle bank\n",
    "        rec_particle_event_table = get_event_table(rec_particle_keys,event_num,batch,dtype=float)\n",
    "        \n",
    "        # Get REC::Traj bank\n",
    "        rec_traj_event_table = get_event_table(rec_traj_keys,event_num,batch,dtype=float)\n",
    "        \n",
    "        # Get REC::Kinematics bank\n",
    "        rec_kinematics_event_table = get_event_table(rec_kinematics_keys,event_num,batch,dtype=float)\n",
    "        \n",
    "        # Get MC::Lund bank and MC->REC matching indices\n",
    "        mc_lund_event_table = get_event_table(mc_lund_keys,event_num,batch,dtype=float)\n",
    "        match_indices  = get_match_indices(rec_particle_event_table,mc_lund_event_table)#TODO: This is somehow resetting rec_particle_event_table...\n",
    "        \n",
    "        # Check MC::Lund matched indices for Lambda decay and set event label accordingly\n",
    "        has_decay, rec_indices = check_has_decay(rec_particle_event_table,mc_lund_event_table,match_indices,decay,\n",
    "                                   rec_particle_pid_idx=0,mc_lund_pid_idx=3,mc_lund_parent_idx=4,mc_lund_daughter_idx=5)#NOTE: TODO: Decay should be formatted as follows: [parent_pid,[daughter_pid1,daughter_pid2,...]]\n",
    "        y         = [1] if has_decay==True else [0]\n",
    "\n",
    "#         ----------------------------------------------------------------------#\n",
    "#         TODO: Here have 3 options:\n",
    "#         1. Create pyg graphs EVENT-BY-EVENT                                      -> GNN\n",
    "#         2. Create awkward arrays of dim (nPions,nBanks(X)nEntries,nCols->PADDED) -> CNN\n",
    "#         3. Create awkward arrays of dim (nPions,nBanks(X)nEntries*nCols->PADDED) -> NN\n",
    "#         ----------------------------------------------------------------------#\n",
    "        \n",
    "        # Select requested rows from REC::Particle data\n",
    "        rec_particle_event_x = get_sub_array(rec_particle_event_table,rec_particle_entry_indices)\n",
    "        \n",
    "        #TODO: Select which kinematics correspond to MC matched particles...\n",
    "        #NOTE: This is important because...GNN doesn't tell you which to use though... just tells you if there is a match anywhere...so maybe not necessary...\n",
    "        \n",
    "#         print(\"DEBUGGING: rec_particle_event_table = \",rec_particle_event_table)\n",
    "#         print(\"DEBUGGING: rec_particle_event_x     = \",rec_particle_event_x)\n",
    "#         print(\"DEBUGGING: mc_lund_event_table      = \",mc_lund_event_table)\n",
    "#         print(\"DEBUGGING: has_decay                = \",has_decay)\n",
    "#         print(\"DEBUGGING: y                        = \",y)\n",
    "        \n",
    "#         # Preprocess data\n",
    "#         x = preprocess(rec_particle_event_x)\n",
    "#         x = torch.tensor(x,dtype=torch.float32)\n",
    "# #         print(\"DEBUGGING: x = \",x)\n",
    "\n",
    "        # Preprocess data from REC::Traj instead #NOTE: This also automatically selects subarray\n",
    "        x = preprocess_rec_traj(rec_traj_event_table)\n",
    "        x = torch.tensor(x,dtype=torch.float32)\n",
    "        if torch.abs(x).max()>1.0:\n",
    "            print(\"DEBUGGING: torch.where(torch.abs(x)>1.0) = \",torch.where(torch.abs(x)>1.0))\n",
    "            print(\"DEBUGGING: torch.abs(x).max()            = \",torch.abs(x).max())\n",
    "        \n",
    "        # Define edge index\n",
    "        num_nodes  = len(x)\n",
    "        edge_index = torch.tensor([[i,j] for i in range(num_nodes) for j in range(num_nodes)],dtype=torch.long)\n",
    "#         print(\"DEBUGGING: edge_index = \",edge_index)\n",
    "#         print(\"DEBUGGING: edge_index.t().contiguous() = \",edge_index.t().contiguous())\n",
    "        \n",
    "        # Create PyG graph\n",
    "        data             = tg.data.Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "        data.y           = torch.tensor(y,dtype=torch.long) #NOTE: Add extra dimension here so that training gets target batch dimension right.\n",
    "        data.kinematics  = torch.tensor(rec_kinematics_event_table,dtype=torch.float32)\n",
    "        data.rec_indices = torch.tensor(rec_indices,dtype=torch.long)\n",
    "        \n",
    "        # Add graph to dataset\n",
    "        datalist.append(data)\n",
    "        \n",
    "#         #DEBUGGING: BEGIN\n",
    "#         print(data)\n",
    "#         g = tg.utils.to_networkx(data,to_undirected=True)\n",
    "#         import networkx as nx\n",
    "#         node_labels = {i:int(val.item()*100) for i, val in enumerate(data.x[:,5])}\n",
    "#         nx.draw(g,labels=node_labels)\n",
    "#         #DEBUGGING: END\n",
    "        \n",
    "#         break#DEBUGGING\n",
    "        \n",
    "#     break#DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "seeing-routine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "\n",
    "\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None, datalist=[]):\n",
    "        self.datalist = datalist\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.load(self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = self.datalist\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        self.save(data_list, self.processed_paths[0])\n",
    "        # For PyG<2.4:\n",
    "        # torch.save(self.collate(data_list), self.processed_paths[0])\n",
    "\n",
    "# Create PyG Dataset\n",
    "dataset = MyOwnDataset(\n",
    "            '/work/clas12/users/mfmce/pyg_test_rec_traj_dataset_1_8_23/',\n",
    "            transform=None,\n",
    "            pre_transform=None,\n",
    "            pre_filter=None,\n",
    "            datalist=datalist\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "czech-excuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 265M\n",
      "-rw-r--r-- 1 mfmce nogroup 2.2G Jan  8 17:27 data.pt\n",
      "-rw-r--r-- 1 mfmce nogroup  888 Jan  8 17:27 pre_transform.pt\n",
      "-rw-r--r-- 1 mfmce nogroup  876 Jan  8 17:27 pre_filter.pt\n"
     ]
    }
   ],
   "source": [
    "root = '/work/clas12/users/mfmce/pyg_test_rec_traj_dataset_1_8_23/'\n",
    "### !rm -rf $root/\n",
    "!mkdir -p $root\n",
    "!ls -lrth $root/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tested-sharp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUGGING: sg_count     =  5076\n",
      "DEBUGGING: len(dataset) =  48382\n",
      "DEBUGGING: sg_fraction  =  0.10491505105204414\n"
     ]
    }
   ],
   "source": [
    "sg_count = 0\n",
    "for g in dataset:\n",
    "    if g.y[0].item() == 1: sg_count += 1\n",
    "        \n",
    "print(\"DEBUGGING: sg_count     = \",sg_count)\n",
    "print(\"DEBUGGING: len(dataset) = \",len(dataset))\n",
    "print(\"DEBUGGING: sg_fraction  = \",sg_count/len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "lyric-purse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_\n"
     ]
    }
   ],
   "source": [
    "DEBUGGING: sg_count     =  5076\n",
    "DEBUGGING: len(dataset) =  48382\n",
    "DEBUGGING: sg_fraction  =  0.10491505105204414"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "insured-judgment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-technique",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "periodic-appointment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 278])\n",
      "Shape of y: torch.Size([64, 6]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, drop_last=True) #NOTE: IMPORTANT TO SET DROP_LAST=TRUE IF DATASET IS NOT PERFECTLY MATCHED WITH BATCH_SIZE\n",
    "val_dataloader   = DataLoader(val_data,   batch_size=batch_size, drop_last=True)\n",
    "test_dataloader  = DataLoader(test_data,  batch_size=batch_size, drop_last=True)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "    \n",
    "# print(\"DEBUGGING: len(test_dataloader.dataset) = \",len(test_dataloader.dataset))\n",
    "# print(\"DEBUGGING: test_dataloader.dataset[0][1] = \",test_dataloader.dataset[0][1])\n",
    "# print(\"DEBUGGING: len(torch.tensor([el[1] for el in test_dataloader.dataset])) = \",len(torch.tensor([el[1] for el in test_dataloader.dataset])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wired-windows",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6871948\n",
      "858993\n",
      "858993\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "intelligent-interest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "DEBUGGING: input_dim =  278\n",
      "DEBUGGING: output_dim =  6\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=278, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=128, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "#         print(\"DEBUGGING: in model.forward(): x.dtype = \",x.dtype)\n",
    "#         print(\"DEBUGGING: type(model.parameters()) = \",type(model.parameters()))\n",
    "#         print(\"DEBUGGING: dtypes of model.parameters() = \",[ el.dtype for el in list(model.parameters())])\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "input_dim  = dataset[0][0].shape[-1]\n",
    "output_dim = dataset[0][1].shape[-1]\n",
    "print(\"DEBUGGING: input_dim = \",input_dim)\n",
    "print(\"DEBUGGING: output_dim = \",output_dim)\n",
    "model = NeuralNetwork(input_dim,output_dim).to(device)#.to(torch.float32) <- default for torch model weights is torch.float32\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hydraulic-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR, MultiStepLR, ReduceLROnPlateau\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n",
    "scheduler2 = MultiStepLR(optimizer, milestones=[10,20], gamma=0.1)\n",
    "schedulers = [scheduler2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "velvet-emerald",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()['param_groups'][0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "healthy-seating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, epoch_idx):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "#         print(\"DEBUGGING: batch = \",batch)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "#         print(\"DEBUGGING: X.dtype = \",X.dtype)\n",
    "#         print(\"DEBUGGING: np.array(model.weights)\",np.array(model.weights).dtype)\n",
    "        if torch.any(torch.isnan(X)):\n",
    "            print(\"FOUND NAN IN INPUT\")\n",
    "        if torch.any(torch.isnan(y)):\n",
    "            print(\"FOUND NAN IN LABEL\")\n",
    "        if X.shape[0]!=batch_size:\n",
    "            print(\"DEBUGGING: X.shape = \",X.shape)\n",
    "            print(\"DEBUGGING: y.shape = \",y.shape)\n",
    "#         print(\"DEBUGGING: X.dtype = \",X.dtype)\n",
    "#         print(\"DEBUGGING: y.dtype = \",y.dtype)\n",
    "#         print(\"DEBUGGING: list(model.parameters())[0]       = \",list(model.parameters())[0])\n",
    "#         print(\"DEBUGGING: list(model.parameters())[0].dtype = \",list(model.parameters())[0].dtype)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"\\rEpoch {epoch_idx}: Training loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\",end=\"\")\n",
    "            losses.append(loss)\n",
    "            \n",
    "    mean_loss = np.mean(losses)\n",
    "    print(f\"\\rEpoch {epoch_idx}: Training loss: {mean_loss:>7f}  [{current:>5d}/{size:>5d}]\",end=\"\\n\")\n",
    "    \n",
    "    lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    \n",
    "    for s in schedulers:\n",
    "        s.step()\n",
    "    return mean_loss, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "brutal-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(dataloader, model, loss_fn, epoch_idx):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    fractional_diffs = torch.zeros(output_dim).to(device)\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "#             print(\"DEBUGGING: X.shape = \",X.shape)\n",
    "#             print(\"DEBUGGING: y.shape = \",y.shape)\n",
    "#             print(\"DEBUGGING: X.dtype = \",X.dtype)\n",
    "#             print(\"DEBUGGING: y.dtype = \",y.dtype)\n",
    "#             print(\"DEBUGGING: list(model.parameters())[0]       = \",list(model.parameters())[0])\n",
    "#             print(\"DEBUGGING: list(model.parameters())[0].dtype = \",list(model.parameters())[0].dtype)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "#             correct = torch.add(fractional_diffs,torch.divide((pred - y),y))\n",
    "    test_loss /= num_batches\n",
    "#     correct /= size\n",
    "    print(f\"Epoch {epoch_idx}: Validation loss: {test_loss:>8f}\")\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-courage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training loss: 0.000849  [6867264/6871948]\n",
      "Epoch 0: Validation loss: 0.000681\n",
      "Epoch 1: Training loss: 0.000681  [6867264/6871948]\n",
      "Epoch 1: Validation loss: 0.000672\n",
      "Epoch 2: Training loss: 0.000670  [6867264/6871948]\n",
      "Epoch 2: Validation loss: 0.000658\n",
      "Epoch 3: Training loss: 0.000663  [6867264/6871948]\n",
      "Epoch 3: Validation loss: 0.000658\n",
      "Epoch 4: Training loss: 0.000657  [6867264/6871948]\n",
      "Epoch 4: Validation loss: 0.000656\n",
      "Epoch 5: Training loss: 0.000656  [6867264/6871948]\n",
      "Epoch 5: Validation loss: 0.000655\n",
      "Epoch 6: Training loss: 0.000656  [6867264/6871948]\n",
      "Epoch 6: Validation loss: 0.000661\n",
      "Epoch 7: Training loss: 0.000653  [6867264/6871948]\n",
      "Epoch 7: Validation loss: 0.000652\n",
      "Epoch 8: Training loss: 0.000651  [6867264/6871948]\n",
      "Epoch 8: Validation loss: 0.000648\n",
      "Epoch 9: Training loss: 0.000652  [6867264/6871948]\n",
      "Epoch 9: Validation loss: 0.000655\n",
      "Epoch 10: Training loss: 0.000624  [6867264/6871948]\n",
      "Epoch 10: Validation loss: 0.000618\n",
      "Epoch 11: Training loss: 0.000623  [6867264/6871948]\n",
      "Epoch 11: Validation loss: 0.000617\n",
      "Epoch 12: Training loss: 0.000622  [6867264/6871948]\n",
      "Epoch 12: Validation loss: 0.000616\n",
      "Epoch 13: Training loss: 0.000621  [6867264/6871948]\n",
      "Epoch 13: Validation loss: 0.000616\n",
      "Epoch 14: Training loss: 0.000621  [6867264/6871948]\n",
      "Epoch 14: Validation loss: 0.000615\n",
      "Epoch 15: Training loss: 0.000621  [6867264/6871948]\n",
      "Epoch 15: Validation loss: 0.000615\n",
      "Epoch 16: Training loss: 0.000620  [6867264/6871948]\n",
      "Epoch 16: Validation loss: 0.000615\n",
      "Epoch 17: Training loss: 0.000620  [6867264/6871948]\n",
      "Epoch 17: Validation loss: 0.000614\n",
      "Epoch 18: Training loss: 0.000619  [6867264/6871948]\n",
      "Epoch 18: Validation loss: 0.000615\n",
      "Epoch 19: Training loss: 0.000619  [6867264/6871948]\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "lrs = []\n",
    "for epoch_idx in range(epochs):\n",
    "#     print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss, lr = train(train_dataloader, model, loss_fn, optimizer, epoch_idx)\n",
    "    val_loss = val(val_dataloader, model, loss_fn, epoch_idx)\n",
    "#     print(\"DEBUGGING: type(train_loss) = \",type(train_loss))\n",
    "#     print(\"DEBUGGING: type(val_loss) = \",type(val_loss))\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    lrs.append(lr)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8,5))\n",
    "plt.semilogy([i for i in range(len(train_losses))],train_losses,color='tab:blue',label='Training Loss')\n",
    "plt.semilogy([i for i in range(len(train_losses))],val_losses,color='tab:orange',label='Validation Loss')\n",
    "plt.ylim((5e-4,1e-3))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(loc='best')\n",
    "f.savefig('training_metrics.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(\"DEBUGGING: size = \",size)\n",
    "    num_batches = len(dataloader)\n",
    "    print(\"DEBUGGING: num_batches = \",num_batches)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    inputs = None\n",
    "    preds = None\n",
    "    tests = None\n",
    "    correct = None\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "#             print(\"DEBUGGING: X.shape = \",X.shape)\n",
    "#             print(\"DEBUGGING: y.shape = \",y.shape)\n",
    "#             print(\"DEBUGGING: X.dtype = \",X.dtype)\n",
    "#             print(\"DEBUGGING: y.dtype = \",y.dtype)\n",
    "#             print(\"DEBUGGING: list(model.parameters())[0]       = \",list(model.parameters())[0])\n",
    "#             print(\"DEBUGGING: list(model.parameters())[0].dtype = \",list(model.parameters())[0].dtype)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            inputs = torch.concatenate((inputs,X),axis=0) if inputs is not None else X\n",
    "            preds = torch.concatenate((preds,pred),axis=0) if preds is not None else pred\n",
    "            tests = torch.concatenate((tests,y),axis=0) if tests is not None else y\n",
    "            correct = torch.concatenate((correct,pred - y),axis=0) if correct is not None else pred - y\n",
    "    test_loss /= num_batches\n",
    "#     correct /= size\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    # Copy to CPU\n",
    "    inputs = inputs.cpu()\n",
    "    correct = correct.cpu()\n",
    "    preds = preds.cpu()\n",
    "    tests = tests.cpu()\n",
    "    \n",
    "    print(\"DEBUGGING: inputs.shape = \",inputs.shape)\n",
    "    print(\"DEBUGGING: correct.shape = \",correct.shape)\n",
    "    print(\"DEBUGGING: preds.shape = \",preds.shape)\n",
    "    print(\"DEBUGGING: tests.shape = \",tests.shape)\n",
    "    print()\n",
    "    print(\"DEBUGGING: correct.min() = \",correct.min().item())\n",
    "    print(\"DEBUGGING: preds.min() = \",preds.min().item())\n",
    "    print(\"DEBUGGING: tests.min() = \",tests.min().item())\n",
    "    print()\n",
    "    print(\"DEBUGGING: correct.max() = \",correct.max().item())\n",
    "    print(\"DEBUGGING: preds.max() = \",preds.max().item())\n",
    "    print(\"DEBUGGING: tests.max() = \",tests.max().item())\n",
    "    print()\n",
    "    print(\"DEBUGGING: correct.mean() = \",correct.mean().item())\n",
    "    print(\"DEBUGGING: preds.mean() = \",preds.mean().item())\n",
    "    print(\"DEBUGGING: tests.mean() = \",tests.mean().item())\n",
    "    print()\n",
    "    print(\"DEBUGGING: correct.std() = \",correct.std().item())\n",
    "    print(\"DEBUGGING: preds.std() = \",preds.std().item())\n",
    "    print(\"DEBUGGING: tests.std() = \",tests.std().item())\n",
    "    \n",
    "    low_high = (-10,10)\n",
    "    \n",
    "    # Plot differences between test and prediction\n",
    "    figs = []\n",
    "    for j in range(0,3):\n",
    "        figsize = (8,5)\n",
    "        f0 = plt.figure(figsize=figsize)\n",
    "        a0 = plt.hist((inputs[:,j]*10),bins=100,range=low_high)\n",
    "        plt.xlabel('inputs '+dataset.data_keys[j])\n",
    "        f0.savefig('inputs_'+dataset.data_keys[j].replace(\"::\",\"_\")+'.pdf')\n",
    "        \n",
    "        figsize = (8,5)\n",
    "        f1 = plt.figure(figsize=figsize)\n",
    "        a1 = plt.hist((preds[:,j]*10),bins=100,range=low_high)\n",
    "        plt.xlabel('preds '+dataset.data_keys[j])\n",
    "        f1.savefig('preds_'+dataset.data_keys[j].replace(\"::\",\"_\")+'.pdf')\n",
    "\n",
    "        figsize = (8,5)\n",
    "        f2 = plt.figure(figsize=figsize)\n",
    "        a2 = plt.hist((tests[:,j]*10),bins=100,range=low_high)\n",
    "        plt.xlabel(dataset.truth_keys[j])\n",
    "        f2.savefig(dataset.truth_keys[j].replace(\"::\",\"_\")+'.pdf')\n",
    "\n",
    "        figsize = (8,5)\n",
    "        f3 = plt.figure(figsize=figsize)\n",
    "        a3 = plt.hist((correct[:,j]*10),bins=100,range=low_high)\n",
    "        plt.xlabel(dataset.data_keys[j]+' - '+dataset.truth_keys[j])\n",
    "        f3.savefig('dif_'+dataset.data_keys[j].replace(\"::\",\"_\")+'.pdf')\n",
    "        \n",
    "        # Print stuff\n",
    "        print(\"DEBUGGING: target label = \",dataset.truth_keys[j])\n",
    "        print(\"DEBUGGING: mean Delta   = \",correct[:,j].mean()*10)\n",
    "        print(\"DEBUGGING: std Delta    = \",correct[:,j].std()*10)\n",
    "        \n",
    "        # Get original diffs\n",
    "        original_deltas = np.subtract(inputs[:,j],tests[:,j])\n",
    "        print(\"DEBUGGING: mean orig Delta   = \",original_deltas.mean()*10)\n",
    "        print(\"DEBUGGING: std orig Delta    = \",original_deltas.std()*10)\n",
    "        \n",
    "        figs.extend([f0,f1,f2,f3])\n",
    "    \n",
    "    low_high = (-100,100)\n",
    "    \n",
    "    # Plot differences between test and prediction\n",
    "    for j in range(3,6):\n",
    "        figsize = (8,5)\n",
    "        f0 = plt.figure(figsize=figsize)\n",
    "        a0 = plt.hist((inputs[:,j]*50)-25,bins=100,range=low_high)\n",
    "        plt.xlabel('inputs '+dataset.data_keys[j])\n",
    "        f0.savefig('inputs_'+dataset.data_keys[j].replace(\"::\",\"_\")+'.pdf')\n",
    "        \n",
    "        figsize = (8,5)\n",
    "        f1 = plt.figure(figsize=figsize)\n",
    "        a1 = plt.hist((preds[:,j]*50)-25,bins=100,range=low_high)\n",
    "        plt.xlabel('preds '+dataset.data_keys[j])\n",
    "        f1.savefig('preds_'+dataset.data_keys[j].replace(\"::\",\"_\")+'.pdf')\n",
    "\n",
    "        figsize = (8,5)\n",
    "        f2 = plt.figure(figsize=figsize)\n",
    "        a2 = plt.hist((tests[:,j]*50)-25,bins=100,range=low_high)\n",
    "        plt.xlabel(dataset.truth_keys[j])\n",
    "        f2.savefig(dataset.truth_keys[j].replace(\"::\",\"_\")+'.pdf')\n",
    "\n",
    "        figsize = (8,5)\n",
    "        f3 = plt.figure(figsize=figsize)\n",
    "        a3 = plt.hist((correct[:,j]*50),bins=100,range=low_high)\n",
    "        plt.xlabel(dataset.data_keys[j]+' - '+dataset.truth_keys[j])\n",
    "        f3.savefig('dif_'+dataset.data_keys[j].replace(\"::\",\"_\")+'.pdf')\n",
    "        \n",
    "        # Print stuff\n",
    "        print(\"DEBUGGING: target label = \",dataset.truth_keys[j])\n",
    "        print(\"DEBUGGING: mean Delta   = \",correct[:,j].mean()*10)\n",
    "        print(\"DEBUGGING: std Delta    = \",correct[:,j].std()*10)\n",
    "        \n",
    "        # Get original diffs\n",
    "        original_deltas = np.subtract(inputs[:,j],tests[:,j])\n",
    "        print(\"DEBUGGING: mean orig Delta   = \",original_deltas.mean()*10)\n",
    "        print(\"DEBUGGING: std orig Delta    = \",original_deltas.std()*10)\n",
    "        \n",
    "        figs.extend([f0,f1,f2,f3])\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-porcelain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "biological-philadelphia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4908\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-separate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_cuda",
   "language": "python",
   "name": "venv_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
